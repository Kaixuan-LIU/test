import os
import json
from collections import defaultdict
from typing import List, Dict
import jieba
from Agent_builder import AgentBuilder
from api_handler import ChatFireAPIClient
from database import MySQLDB, DB_CONFIG
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer


class AgentGenerator:
    def __init__(self, world_descriptions: List[str], api_key: str, db_config: dict, base_template: str = None):
        """
        æ™ºèƒ½ä½“æ‰¹é‡ç”Ÿæˆå™¨
        å‚æ•°:
            world_descriptions: ä¸–ç•Œè§‚æè¿°åˆ—è¡¨
            api_key: APIå¯†é’¥
            db_config: æ•°æ®åº“é…ç½®
            base_template: åŸºç¡€æ¨¡æ¿
        """
        self.world_descriptions = world_descriptions
        self.api_key = api_key
        self.db = MySQLDB(**db_config)
        self.base_template = base_template or """
        è¯·æ ¹æ®ä»¥ä¸‹ä¸–ç•Œè§‚æè¿°åˆ›å»ºæ™ºèƒ½ä½“:
        {world_description}

        è¦æ±‚:
        - æ™ºèƒ½ä½“éœ€åŒ…å«: å§“åã€èŒä¸šã€æ ¸å¿ƒæ€§æ ¼ç‰¹å¾
        - æ·»åŠ 3ä¸ªç‹¬ç‰¹ä¸ªæ€§æ ‡ç­¾
        - åŒ…å«1ä¸ªä¸“ä¸šçŸ¥è¯†é¢†åŸŸ
        """

    def generate_agents(self, num_per_world: int = 3):
        """æ‰¹é‡ç”Ÿæˆæ™ºèƒ½ä½“"""
        all_agents = []

        for world_desc in self.world_descriptions:
            for _ in range(num_per_world):
                # ç”Ÿæˆæ™ºèƒ½ä½“é…ç½®
                agent_config = self._generate_agent_config(world_desc)

                # åˆ›å»ºæ™ºèƒ½ä½“
                builder = AgentBuilder(api_key=self.api_key)
                agent_data = builder.build_agent(agent_config)
                metadata = self._create_metadata(agent_data, world_desc)
                all_agents.append(metadata)

        return all_agents

    def _generate_agent_config(self, world_desc: str) -> str:
        """ä½¿ç”¨AIç”Ÿæˆæ™ºèƒ½ä½“é…ç½®æè¿°"""
        prompt = self.base_template.format(world_description=world_desc)

        # è°ƒç”¨APIç”Ÿæˆé…ç½®
        response = ChatFireAPIClient(api_key=self.api_key).call_api(
            [{"role": "user", "content": prompt}],
            max_tokens=300
        )

        return response['choices'][0]['message']['content']

    def auto_classify(self, agent_metadatas: List[Dict]) -> Dict:
        """é€šè¿‡å…³é”®è¯æå–ç›´æ¥åˆ†ç±»æ™ºèƒ½ä½“"""
        # æå–æ‰€æœ‰æ™ºèƒ½ä½“çš„ç‰¹å¾æ–‡æœ¬
        texts = [
            f"{m['basic']['profession']} {' '.join(m['basic']['personality_tags'])} {' '.join(m['basic']['knowledge_domains'])}"
            for m in agent_metadatas
        ]

        # æå–å…³é”®è¯å¹¶åˆ›å»ºåˆ†ç±»
        classified_agents = self._keyword_classification(texts, agent_metadatas)

        # åˆå¹¶ç›¸ä¼¼ç±»åˆ«
        merged_classification = self._merge_similar_categories(classified_agents)

        return merged_classification

    def _keyword_classification(self, texts: List[str], agents: List[Dict]) -> Dict:
        """æ ¸å¿ƒå…³é”®è¯åˆ†ç±»é€»è¾‘"""
        # ç‰¹å¾æå–ï¼šè·å–æ¯ä¸ªæ–‡æœ¬çš„å‰3ä¸ªå…³é”®è¯
        vectorizer = TfidfVectorizer(tokenizer=jieba.cut, stop_words=self._get_stop_words())
        tfidf_matrix = vectorizer.fit_transform(texts)
        feature_names = vectorizer.get_feature_names_out()

        # åˆ›å»ºç±»åˆ«å­—å…¸
        categories = defaultdict(list)

        # ä¸ºæ¯ä¸ªæ™ºèƒ½ä½“åˆ†é…ç±»åˆ«
        for i, agent in enumerate(agents):
            # è·å–å½“å‰æ–‡æ¡£çš„TF-IDFå‘é‡
            row = tfidf_matrix[i].toarray()[0]

            # è·å–TF-IDFæœ€é«˜çš„å‰3ä¸ªå…³é”®è¯
            top_keyword_indices = np.argsort(row)[::-1][:3]
            top_keywords = [feature_names[idx] for idx in top_keyword_indices if row[idx] > 0]

            if not top_keywords:
                categories['å…¶ä»–'].append(agent)
                continue

            # åˆ›å»ºå¤åˆç±»åˆ«æ ‡ç­¾
            category_label = '_'.join(top_keywords)
            categories[category_label].append(agent)

        return dict(categories)

    def _merge_similar_categories(self, classified_dict: Dict) -> Dict:
        """åˆå¹¶ç›¸ä¼¼ç±»åˆ«"""
        # åˆ›å»ºç›¸ä¼¼åº¦æ˜ å°„è¡¨
        merged_dict = {}
        category_mapping = {}

        for category in list(classified_dict.keys()):
            if not category:
                continue

            # æå–æ ¸å¿ƒè¯æ ¹ï¼ˆå–æ¯ä¸ªå…³é”®è¯çš„ç¬¬ä¸€ä¸ªå­—ï¼‰
            core_roots = ''.join([word[0] for word in category.split('_') if word])

            # æŸ¥æ‰¾ç›¸ä¼¼ç±»åˆ«
            merged = False
            for existing in merged_dict:
                # ç›¸ä¼¼æ€§åˆ¤æ–­ï¼šå…±äº«æ ¸å¿ƒè¯æ ¹æˆ–åŒ…å«å…³ç³»
                shared_roots = sum(1 for char in core_roots if char in existing) >= 2
                contains = core_roots in existing or existing in core_roots

                if shared_roots or contains:
                    merged_dict[existing].extend(classified_dict[category])
                    category_mapping[category] = existing
                    merged = True
                    break

            if not merged:
                merged_dict[core_roots] = classified_dict[category]
                category_mapping[category] = core_roots

        # é‡å‘½åç±»åˆ«ä¸ºå¯è¯»æ€§æ›´å¥½çš„æ ‡ç­¾
        final_classification = {}
        for category, agents in merged_dict.items():
            # è·å–æœ€å¸¸å‡ºç°çš„ä¸‰ä¸ªå…³é”®è¯ä½œä¸ºæœ€ç»ˆæ ‡ç­¾
            all_keywords = []
            for agent in agents:
                profession = agent['basic']['profession']
                tags = agent['basic']['personality_tags']
                domains = agent['basic']['knowledge_domains']

                all_keywords.extend(jieba.cut(profession))
                all_keywords.extend(tags)
                all_keywords.extend(domains)

            # ç»Ÿè®¡è¯é¢‘
            word_freq = defaultdict(int)
            for word in all_keywords:
                if len(word) > 1:  # å¿½ç•¥å•å­—è¯
                    word_freq[word] += 1

            # å–æœ€é«˜é¢‘çš„3ä¸ªè¯
            top_keywords = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:3]
            final_label = '|'.join([word for word, _ in top_keywords])

            final_classification[final_label] = agents

        return final_classification

    def _get_stop_words(self) -> List[str]:
        """è·å–ä¸­æ–‡åœç”¨è¯è¡¨"""
        return [
            'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°',
            'è¯´', 'è¦',
            'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™', 'ä¸­', 'ä¸º', 'æˆ‘ä»¬', 'ä»–', 'ä¼š', 'ç­‰', 'å¹´', 'æœˆ',
            'æ—¥'
        ]

    def _create_metadata(self, agent_data: Dict, world_desc: str) -> Dict:
        """åˆ›å»ºå…ƒæ•°æ®"""
        # ä»agent_dataä¸­æå–å…³é”®ç‰¹å¾
        return {
            "agent_id": agent_data.get("agent_id", ""),
            "world_description": world_desc,
            "basic": {
                "profession": agent_data.get("èŒä¸š", ""),
                "personality_tags": agent_data.get("æ€§æ ¼æ ‡ç­¾", []),
                "knowledge_domains": agent_data.get("çŸ¥è¯†é¢†åŸŸ", [])
            },
            "interaction": {
                "language_style": agent_data.get("è¯­è¨€é£æ ¼", ""),
                "core_prompt": agent_data.get("æ ¸å¿ƒæç¤ºè¯", "")
            },
            "technical": {
                "generation_version": "v2.0-flexible",
                "api_config": {"model": "gpt-4-turbo"}
            }
        }

    def save_agents(self, agents: List[Dict]):
        """ä¿å­˜æ™ºèƒ½ä½“å…ƒæ•°æ®åˆ°æ•°æ®åº“"""
        for agent_meta in agents:
            self._save_metadata(agent_meta)
        print(f"âœ… æˆåŠŸä¿å­˜ {len(agents)} ä¸ªæ™ºèƒ½ä½“å…ƒæ•°æ®")

    def generate_and_save_agents(self, num_per_world: int = 3) -> List[Dict]:
        """æ‰¹é‡ç”Ÿæˆå¹¶ä¿å­˜æ™ºèƒ½ä½“"""
        agents = self.generate_agents(num_per_world)  # ç”Ÿæˆ
        self.save_agents(agents)  # ä¿å­˜
        return agents


def run_generation():
    """è¿è¡Œæ‰¹é‡ç”Ÿæˆ"""
    API_KEY = "sk-Jgb98JXxJ0nNfB2vcNoQ0ZZg1B5zYbM1TgsGmc1LOrNPMIPV"
    DB_CONFIG = {
        "host": "101.200.229.113",
        "user": "gongwei",
        "password": "Echo@123456",
        "database": "echo",
        "port": 3306,
        "charset": "utf8mb4"
    }

    # ä¸–ç•Œè§‚æè¿°
    WORLD_DESCRIPTIONS = [
        "ä¸€ä¸ªåŒ»ç–—ç§‘æŠ€é«˜åº¦å‘è¾¾çš„æœªæ¥ä¸–ç•Œ",
        "ä¸€ä¸ªæ–‡å­¦åˆ›ä½œæˆä¸ºä¸»è¦ç”Ÿäº§åŠ›çš„ç¤¾ä¼š",
        "ä¸€ä¸ªæ³•å¾‹AIä¸»å¯¼å¸æ³•ä½“ç³»çš„æ•°å­—æ—¶ä»£"
    ]

    generator = AgentGenerator(
        world_descriptions=WORLD_DESCRIPTIONS,
        api_key=API_KEY,
        db_config=DB_CONFIG
    )

    # æ‰¹é‡ç”Ÿæˆå¹¶ä¿å­˜æ™ºèƒ½ä½“
    agents = generator.generate_and_save_agents(num_per_world=5)

    # è¿”å›ç”Ÿæˆçš„æ™ºèƒ½ä½“IDåˆ—è¡¨
    agent_ids = [agent["agent_id"] for agent in agents]
    print(f"ğŸ“‹ ç”Ÿæˆçš„æ™ºèƒ½ä½“IDåˆ—è¡¨: {agent_ids}")
    return agent_ids


if __name__ == "__main__":
    agent_ids = run_generation()